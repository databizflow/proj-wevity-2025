# wevity_crawler_improved.py - ê°œì„ ëœ í¬ë¡¤ëŸ¬
from selenium import webdriver
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from webdriver_manager.chrome import ChromeDriverManager
from bs4 import BeautifulSoup
import pandas as pd
from datetime import datetime, timedelta
import re
import time
import logging
from typing import Optional, List, Dict
import requests
from urllib.parse import urljoin

# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

class WevityCrawler:
    """Wevity ê³µëª¨ì „ í¬ë¡¤ëŸ¬ í´ë˜ìŠ¤ - ê°œì„  ë²„ì „"""
    
    def __init__(self, headless=True, timeout=30):
        self.base_url = "https://www.wevity.com"
        self.timeout = timeout
        self.driver = None
        self.headless = headless
        self.session = requests.Session()
        
        # User-Agent ì„¤ì •
        self.headers = {
            'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36'
        }
        self.session.headers.update(self.headers)
        
    def _setup_driver(self):
        """Chrome ë“œë¼ì´ë²„ ì„¤ì • - ê°œì„ ëœ ë²„ì „"""
        options = Options()
        
        # ê¸°ë³¸ ì˜µì…˜
        if self.headless:
            options.add_argument('--headless=new')  # ìƒˆë¡œìš´ headless ëª¨ë“œ
        
        # ì•ˆì •ì„± ê°œì„  ì˜µì…˜ë“¤
        options.add_argument('--no-sandbox')
        options.add_argument('--disable-dev-shm-usage')
        options.add_argument('--disable-gpu')
        options.add_argument('--disable-blink-features=AutomationControlled')
        options.add_argument('--disable-extensions')
        options.add_argument('--disable-plugins')
        options.add_argument('--disable-images')  # ì´ë¯¸ì§€ ë¡œë”© ë¹„í™œì„±í™”ë¡œ ì†ë„ ê°œì„ 
        options.add_argument('--window-size=1920,1080')
        
        # User-Agent ì„¤ì •
        options.add_argument(f'--user-agent={self.headers["User-Agent"]}')
        
        # ìë™í™” ê°ì§€ ë°©ì§€
        options.add_experimental_option("excludeSwitches", ["enable-automation"])
        options.add_experimental_option('useAutomationExtension', False)
        
        # í˜ì´ì§€ ë¡œë”© ì „ëµ
        options.page_load_strategy = 'eager'  # DOMì´ ì¤€ë¹„ë˜ë©´ ë°”ë¡œ ì§„í–‰
        
        try:
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=options)
            
            # ìë™í™” ê°ì§€ ë°©ì§€ ìŠ¤í¬ë¦½íŠ¸
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            # íƒ€ì„ì•„ì›ƒ ì„¤ì •
            self.driver.set_page_load_timeout(self.timeout)
            self.driver.implicitly_wait(10)
            
            logger.info("Chrome ë“œë¼ì´ë²„ê°€ ì„±ê³µì ìœ¼ë¡œ ì„¤ì •ë˜ì—ˆìŠµë‹ˆë‹¤.")
            return True
            
        except Exception as e:
            logger.error(f"ë“œë¼ì´ë²„ ì„¤ì • ì‹¤íŒ¨: {e}")
            return False
    
    def _extract_deadline(self, text: str) -> Optional[datetime]:
        """í…ìŠ¤íŠ¸ì—ì„œ ë§ˆê°ì¼ ì¶”ì¶œ - ê°œì„ ëœ ì •ê·œì‹"""
        if not text:
            return None
            
        # í…ìŠ¤íŠ¸ ì •ë¦¬
        text = re.sub(r'\s+', ' ', text.strip())
        
        # ë§ˆê°ëœ ê³µëª¨ì „ í‚¤ì›Œë“œ ì²´í¬ (ë¨¼ì € ì²´í¬í•´ì„œ ë¹ ë¥´ê²Œ ì œì™¸)
        if any(keyword in text.lower() for keyword in ['ë§ˆê°ë¨', 'ì ‘ìˆ˜ë§ˆê°', 'ì¢…ë£Œë¨', 'ì™„ë£Œë¨']):
            return datetime(2020, 1, 1)  # ê³¼ê±° ë‚ ì§œë¡œ ì„¤ì •í•˜ì—¬ í•„í„°ë§ë˜ë„ë¡
        
        # 1. ì ‘ìˆ˜ê¸°ê°„ í˜•íƒœ íŒ¨í„´ë“¤
        period_patterns = [
            # 2025.01.01 ~ 2025.03.15
            r'(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})\s*[~\-ê¹Œì§€]\s*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            # 25.01.01 ~ 25.03.15
            r'(\d{2}[.\-/]\d{1,2}[.\-/]\d{1,2})\s*[~\-ê¹Œì§€]\s*(\d{2}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            # 1ì›” 1ì¼ ~ 3ì›” 15ì¼
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*[~\-ê¹Œì§€]\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
            # 2025ë…„ 1ì›” 1ì¼ ~ 2025ë…„ 3ì›” 15ì¼
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*[~\-ê¹Œì§€]\s*(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼',
        ]
        
        for pattern in period_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    if 'ë…„' in pattern:  # 2025ë…„ 1ì›” 1ì¼ í˜•íƒœ
                        end_year = int(match.group(4))
                        end_month = int(match.group(5))
                        end_day = int(match.group(6))
                        return datetime(end_year, end_month, end_day)
                    elif 'ì›”' in pattern:  # í•œê¸€ ë‚ ì§œ í˜•íƒœ
                        end_month = int(match.group(3))
                        end_day = int(match.group(4))
                        current_year = datetime.now().year
                        return datetime(current_year, end_month, end_day)
                    else:
                        end_date_str = match.group(2)
                        return self._parse_date_string(end_date_str)
                except (ValueError, IndexError):
                    continue
        
        # 2. ë‹¨ì¼ ë§ˆê°ì¼ í˜•íƒœ
        single_patterns = [
            r'ë§ˆê°\s*:?\s*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            r'ê¹Œì§€\s*:?\s*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            r'(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})\s*ë§ˆê°',
            r'ì ‘ìˆ˜ë§ˆê°\s*:?\s*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            r'(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*ë§ˆê°',
            r'(\d{4})ë…„\s*(\d{1,2})ì›”\s*(\d{1,2})ì¼\s*ë§ˆê°',
            r'ë§ˆê°ì¼\s*:?\s*(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
        ]
        
        for pattern in single_patterns:
            match = re.search(pattern, text)
            if match:
                try:
                    if 'ë…„' in pattern:
                        year = int(match.group(1))
                        month = int(match.group(2))
                        day = int(match.group(3))
                        return datetime(year, month, day)
                    elif 'ì›”' in pattern:
                        month = int(match.group(1))
                        day = int(match.group(2))
                        current_year = datetime.now().year
                        return datetime(current_year, month, day)
                    else:
                        return self._parse_date_string(match.group(1))
                except (ValueError, IndexError):
                    continue
        
        # 3. D-day í˜•íƒœ
        d_match = re.search(r'D[ï¼\-](\d+)', text)
        if d_match:
            days_left = int(d_match.group(1))
            return datetime.now() + timedelta(days=days_left)
        
        # 4. ë‹¨ìˆœ ë‚ ì§œ í˜•íƒœ (ë§ˆì§€ë§‰ì— ì²´í¬)
        simple_date_patterns = [
            r'(\d{4}[.\-/]\d{1,2}[.\-/]\d{1,2})',
            r'(\d{2}[.\-/]\d{1,2}[.\-/]\d{1,2})',
        ]
        
        for pattern in simple_date_patterns:
            matches = re.findall(pattern, text)
            if matches:
                # ì—¬ëŸ¬ ë‚ ì§œê°€ ìˆìœ¼ë©´ ê°€ì¥ ëŠ¦ì€ ë‚ ì§œë¥¼ ë§ˆê°ì¼ë¡œ ê°„ì£¼
                dates = []
                for match in matches:
                    parsed = self._parse_date_string(match)
                    if parsed and parsed.year >= datetime.now().year:
                        dates.append(parsed)
                
                if dates:
                    return max(dates)
        
        return None
    
    def _parse_date_string(self, date_str: str) -> Optional[datetime]:
        """ë‚ ì§œ ë¬¸ìì—´ì„ datetime ê°ì²´ë¡œ ë³€í™˜"""
        if not date_str:
            return None
            
        # êµ¬ë¶„ì í†µì¼
        date_str = re.sub(r'[.\-/]', '.', date_str)
        
        # ë‚ ì§œ í˜•ì‹ë“¤
        formats = [
            '%Y.%m.%d',
            '%y.%m.%d',
            '%Y.%m.%d',
        ]
        
        for fmt in formats:
            try:
                parsed_date = datetime.strptime(date_str, fmt)
                # 2ìë¦¬ ì—°ë„ë¥¼ 4ìë¦¬ë¡œ ë³€í™˜
                if parsed_date.year < 2000:
                    parsed_date = parsed_date.replace(year=parsed_date.year + 2000)
                return parsed_date
            except ValueError:
                continue
        
        return None
    
    def _get_page_with_requests(self, url: str) -> Optional[BeautifulSoup]:
        """requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° (ë¹ ë¥¸ ë°©ë²•)"""
        try:
            response = self.session.get(url, timeout=15)
            response.raise_for_status()
            return BeautifulSoup(response.content, 'html.parser')
        except Exception as e:
            logger.warning(f"requestsë¡œ í˜ì´ì§€ ê°€ì ¸ì˜¤ê¸° ì‹¤íŒ¨: {e}")
            return None
    
    def _extract_contest_info_new_structure(self, item) -> Optional[Dict]:
        """ìƒˆë¡œìš´ HTML êµ¬ì¡°ì— ë§ëŠ” ì •ë³´ ì¶”ì¶œ"""
        try:
            # ë‹¤ì–‘í•œ ì œëª© ì„ íƒì ì‹œë„
            title_selectors = [
                ".tit a", ".title a", "h3 a", "h4 a", 
                ".subject a", ".contest-title a", "a[href*='?c=find&']"
            ]
            
            title_element = None
            for selector in title_selectors:
                title_element = item.select_one(selector)
                if title_element:
                    break
            
            if not title_element:
                # í…ìŠ¤íŠ¸ë§Œ ìˆëŠ” ê²½ìš°
                title_text = item.get_text(strip=True)
                if len(title_text) > 10:  # ìµœì†Œ ê¸¸ì´ ì²´í¬
                    # ë§í¬ ì°¾ê¸°
                    link_element = item.select_one("a[href]")
                    if link_element:
                        return {
                            "ì œëª©": title_text[:100],  # ì œëª© ê¸¸ì´ ì œí•œ
                            "ì£¼ìµœ": "ì£¼ìµœì ì •ë³´ ì—†ìŒ",
                            "ê¸°ê°„": "ê¸°ê°„ ì •ë³´ ì—†ìŒ",
                            "ë§ˆê°ì¼": None,
                            "ë§í¬": urljoin(self.base_url, link_element.get('href', ''))
                        }
                return None
            
            title = title_element.get_text(strip=True)
            href = title_element.get('href', '')
            
            # URL ì •ê·œí™”
            if href.startswith('http'):
                url = href
            else:
                url = urljoin(self.base_url, href)
            
            # ì£¼ìµœì ì •ë³´ ì¶”ì¶œ
            host_selectors = [".organ", ".host", ".organizer", ".company"]
            host = "ì£¼ìµœì ì •ë³´ ì—†ìŒ"
            
            for selector in host_selectors:
                host_element = item.select_one(selector)
                if host_element:
                    host = host_element.get_text(strip=True)
                    break
            
            # ê¸°ê°„ ì •ë³´ ì¶”ì¶œ - ë” ë§ì€ ì„ íƒì ì‹œë„
            period_selectors = [".day", ".period", ".date", ".deadline", ".time", ".dday"]
            period = "ê¸°ê°„ ì •ë³´ ì—†ìŒ"
            deadline = None
            
            # ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œë„ ë‚ ì§œ ì°¾ê¸°
            full_text = item.get_text(strip=True)
            deadline = self._extract_deadline(full_text)
            
            for selector in period_selectors:
                period_element = item.select_one(selector)
                if period_element:
                    period_text = period_element.get_text(strip=True)
                    if period_text:
                        period = period_text
                        # ë” ì •í™•í•œ ë§ˆê°ì¼ ì¶”ì¶œ ì‹œë„
                        extracted_deadline = self._extract_deadline(period_text)
                        if extracted_deadline:
                            deadline = extracted_deadline
                        break
            
            # ë””ë²„ê¹… ë¡œê·¸ ì¶”ê°€
            if deadline:
                logger.debug(f"ì¶”ì¶œëœ ë§ˆê°ì¼: {deadline.date()} - {title[:50]}")
            else:
                logger.debug(f"ë§ˆê°ì¼ ì—†ìŒ: {title[:50]} - ê¸°ê°„: {period}")
            
            # ìƒê¸ˆ ì •ë³´ ì¶”ì¶œ - 1ë“± ìƒê¸ˆ ìš°ì„ 
            prize = "ìƒê¸ˆ ì •ë³´ ì—†ìŒ"
            
            # 1. íŠ¹ì • ìœ„ì¹˜ì˜ 1ë“± ìƒê¸ˆ ì°¾ê¸° (XPath ê¸°ë°˜)
            # li[7]/span ìœ„ì¹˜ì˜ ìƒê¸ˆ ì •ë³´ (1ë“± ìƒê¸ˆ)
            prize_elements = item.select("li span")
            if len(prize_elements) >= 7:
                first_prize_element = prize_elements[6]  # 7ë²ˆì§¸ ìš”ì†Œ (0-based index)
                prize_text = first_prize_element.get_text(strip=True)
                if prize_text and any(char in prize_text for char in ['ì›', 'ë§Œ', 'ì–µ', '$']):
                    prize = f"1ë“±: {prize_text}"
            
            # 2. ì¼ë°˜ì ì¸ ìƒê¸ˆ ì„ íƒìë“¤
            if prize == "ìƒê¸ˆ ì •ë³´ ì—†ìŒ":
                prize_selectors = [".prize", ".reward", ".money", ".won", ".award"]
                for selector in prize_selectors:
                    prize_element = item.select_one(selector)
                    if prize_element:
                        prize_text = prize_element.get_text(strip=True)
                        if prize_text and any(char in prize_text for char in ['ì›', 'ë§Œ', 'ì–µ', '$']):
                            prize = prize_text
                            break
            
            # 3. ì „ì²´ í…ìŠ¤íŠ¸ì—ì„œ 1ë“± ìƒê¸ˆ íŒ¨í„´ ì°¾ê¸°
            if prize == "ìƒê¸ˆ ì •ë³´ ì—†ìŒ":
                first_prize_patterns = [
                    r'1ë“±\s*:?\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                    r'ëŒ€ìƒ\s*:?\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                    r'ìµœìš°ìˆ˜ìƒ\s*:?\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                    r'ê¸ˆìƒ\s*:?\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                ]
                
                for pattern in first_prize_patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        prize = f"1ë“±: {match.group(1)}"
                        break
            
            # 4. ì¼ë°˜ì ì¸ ìƒê¸ˆ íŒ¨í„´ ì°¾ê¸°
            if prize == "ìƒê¸ˆ ì •ë³´ ì—†ìŒ":
                general_prize_patterns = [
                    r'ìƒê¸ˆ\s*:?\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                    r'ì´\s*ìƒê¸ˆ\s*(\d+(?:,\d+)*(?:ë§Œ|ì–µ)?ì›?)',
                    r'(\d+(?:,\d+)*)\s*ë§Œì›',
                    r'(\d+(?:,\d+)*)\s*ì–µì›',
                ]
                
                for pattern in general_prize_patterns:
                    match = re.search(pattern, full_text)
                    if match:
                        prize = match.group(0)
                        break
            
            return {
                "ì œëª©": title,
                "ì£¼ìµœ": host,
                "ê¸°ê°„": period,
                "ë§ˆê°ì¼": deadline.date() if deadline else None,
                "ìƒê¸ˆ": prize,
                "ë§í¬": url
            }
            
        except Exception as e:
            logger.debug(f"ê³µëª¨ì „ ì •ë³´ ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            return None
    
    def _wait_for_page_load(self):
        """í˜ì´ì§€ ë¡œë”© ì™„ë£Œ ëŒ€ê¸°"""
        try:
            # JavaScript ì‹¤í–‰ ì™„ë£Œ ëŒ€ê¸°
            WebDriverWait(self.driver, 10).until(
                lambda driver: driver.execute_script("return document.readyState") == "complete"
            )
            
            # ì¶”ê°€ ëŒ€ê¸° (ë™ì  ì½˜í…ì¸  ë¡œë”©)
            time.sleep(2)
            
            # ê³µëª¨ì „ ëª©ë¡ì´ ìˆëŠ”ì§€ í™•ì¸
            selectors_to_check = [
                "ul.list", ".list", ".contest_list", 
                "[class*='list']", "[class*='item']"
            ]
            
            for selector in selectors_to_check:
                try:
                    WebDriverWait(self.driver, 5).until(
                        EC.presence_of_element_located((By.CSS_SELECTOR, selector))
                    )
                    return True
                except:
                    continue
            
            return False
            
        except Exception as e:
            logger.warning(f"í˜ì´ì§€ ë¡œë”© ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
            return False
    
    def crawl(self, keyword="ê³µê³µë°ì´í„°", max_pages=5, from_date=None, to_date=None) -> pd.DataFrame:
        """ê³µëª¨ì „ ì •ë³´ í¬ë¡¤ë§ - ê°œì„ ëœ ë²„ì „"""
        
        # ë¨¼ì € requestsë¡œ ì‹œë„
        logger.info("requestsë¥¼ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        df_requests = self._crawl_with_requests(keyword, max_pages, from_date, to_date)
        
        if not df_requests.empty:
            logger.info(f"requestsë¡œ {len(df_requests)}ê°œ ê³µëª¨ì „ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
            return df_requests
        
        # requests ì‹¤íŒ¨ ì‹œ Selenium ì‚¬ìš©
        logger.info("Seleniumì„ ì‚¬ìš©í•˜ì—¬ í¬ë¡¤ë§ì„ ì‹œë„í•©ë‹ˆë‹¤...")
        return self._crawl_with_selenium(keyword, max_pages, from_date, to_date)
    
    def _crawl_with_requests(self, keyword, max_pages, from_date, to_date) -> pd.DataFrame:
        """requestsë¥¼ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ë¹ ë¥¸ ë°©ë²•)"""
        results = []
        seen_urls = set()
        
        try:
            for page in range(1, max_pages + 1):
                url = f"https://www.wevity.com/?c=find&s=1&gp={page}&sp=contents&sw={keyword}"
                
                soup = self._get_page_with_requests(url)
                if not soup:
                    continue
                
                # ê³µëª¨ì „ ëª©ë¡ ì°¾ê¸°
                items = self._find_contest_items(soup)
                
                if not items:
                    logger.warning(f"í˜ì´ì§€ {page}: ê³µëª¨ì „ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                page_count = 0
                for item in items:
                    contest_info = self._extract_contest_info_new_structure(item)
                    
                    if not contest_info or contest_info['ë§í¬'] in seen_urls:
                        continue
                    
                    seen_urls.add(contest_info['ë§í¬'])
                    
                    # ë‚ ì§œ í•„í„°ë§
                    if self._filter_by_date(contest_info, from_date, to_date):
                        results.append(contest_info)
                        page_count += 1
                
                logger.info(f"í˜ì´ì§€ {page}: {page_count}ê°œ ê³µëª¨ì „ ìˆ˜ì§‘")
                
                if page_count == 0 and page > 1:
                    break
                    
                time.sleep(1)  # ìš”ì²­ ê°„ê²©
                
        except Exception as e:
            logger.error(f"requests í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        
        return pd.DataFrame(results)
    
    def _crawl_with_selenium(self, keyword, max_pages, from_date, to_date) -> pd.DataFrame:
        """Seleniumì„ ì‚¬ìš©í•œ í¬ë¡¤ë§ (ë°±ì—… ë°©ë²•)"""
        if not self._setup_driver():
            return pd.DataFrame()
        
        results = []
        seen_urls = set()
        
        try:
            for page in range(1, max_pages + 1):
                url = f"https://www.wevity.com/?c=find&s=1&gp={page}&sp=contents&sw={keyword}"
                logger.info(f"í˜ì´ì§€ {page} í¬ë¡¤ë§ ì¤‘: {url}")
                
                self.driver.get(url)
                
                if not self._wait_for_page_load():
                    logger.warning(f"í˜ì´ì§€ {page} ë¡œë”© ì‹¤íŒ¨")
                    continue
                
                soup = BeautifulSoup(self.driver.page_source, 'html.parser')
                items = self._find_contest_items(soup)
                
                if not items:
                    logger.warning(f"í˜ì´ì§€ {page}: ê³µëª¨ì „ ëª©ë¡ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                    continue
                
                page_count = 0
                for item in items:
                    contest_info = self._extract_contest_info_new_structure(item)
                    
                    if not contest_info or contest_info['ë§í¬'] in seen_urls:
                        continue
                    
                    seen_urls.add(contest_info['ë§í¬'])
                    
                    if self._filter_by_date(contest_info, from_date, to_date):
                        results.append(contest_info)
                        page_count += 1
                
                logger.info(f"í˜ì´ì§€ {page}: {page_count}ê°œ ê³µëª¨ì „ ìˆ˜ì§‘")
                
                if page_count == 0 and page > 1:
                    break
                    
        except Exception as e:
            logger.error(f"Selenium í¬ë¡¤ë§ ì¤‘ ì˜¤ë¥˜: {e}")
        finally:
            if self.driver:
                self.driver.quit()
                self.driver = None
        
        return pd.DataFrame(results)
    
    def _find_contest_items(self, soup):
        """ê³µëª¨ì „ ì•„ì´í…œ ì°¾ê¸° - ë‹¤ì–‘í•œ ì„ íƒì ì‹œë„"""
        selectors = [
            "ul.list li",
            ".list li", 
            ".contest_list li",
            ".board_list tr",
            "tr",
            ".item",
            "div[class*='item']",
            "div[class*='contest']",
            "li[class*='list']"
        ]
        
        for selector in selectors:
            items = soup.select(selector)
            # ìœ íš¨í•œ ì•„ì´í…œ ìˆ˜ ì²´í¬ (í—¤ë” ì œì™¸)
            valid_items = [item for item in items if self._is_valid_contest_item(item)]
            
            if len(valid_items) > 0:
                logger.info(f"ì„ íƒì '{selector}': {len(valid_items)}ê°œ ìœ íš¨í•œ ì•„ì´í…œ ë°œê²¬")
                return valid_items
        
        return []
    
    def _is_valid_contest_item(self, item):
        """ìœ íš¨í•œ ê³µëª¨ì „ ì•„ì´í…œì¸ì§€ í™•ì¸"""
        text = item.get_text(strip=True)
        
        # ë„ˆë¬´ ì§§ì€ í…ìŠ¤íŠ¸ëŠ” ì œì™¸
        if len(text) < 10:
            return False
        
        # í—¤ë”ë‚˜ ê´‘ê³  ìš”ì†Œ ì œì™¸
        classes = item.get('class', [])
        if any(cls in ['header', 'top', 'ad', 'banner', 'notice'] for cls in classes):
            return False
        
        # ë§í¬ê°€ ìˆëŠ”ì§€ í™•ì¸
        if not item.select_one('a[href]'):
            return False
        
        return True
    
    def _filter_by_date(self, contest_info, from_date, to_date):
        """ë‚ ì§œ í•„í„°ë§ ë° ë¶ˆí•„ìš”í•œ ê³µëª¨ì „ ì œì™¸"""
        deadline = contest_info['ë§ˆê°ì¼']
        today = datetime.now().date()
        title = contest_info['ì œëª©'][:50]
        full_title = contest_info['ì œëª©'].lower()
        
        # 1. ë¶ˆí•„ìš”í•œ í‚¤ì›Œë“œê°€ í¬í•¨ëœ ê³µëª¨ì „ ì œì™¸
        exclude_keywords = [
            'ëª¨ì§‘', 'ì±„ìš©', 'ë¬´ë£Œ', 'ë©˜í† ë§', 'special', 'ìŠ¤í˜ì…œ',
            'êµìœ¡', 'ê°•ì˜', 'ì„¸ë¯¸ë‚˜', 'ì›Œí¬ìƒµ', 'ì„¤ëª…íšŒ', 'ìƒë‹´',
            'ì§€ì›ì', 'ì°¸ê°€ì', 'ìˆ˜ê°•ìƒ', 'ì¸í„´', 'ì•„ë¥´ë°”ì´íŠ¸',
            'ë´‰ì‚¬', 'ìì›ë´‰ì‚¬', 'ê¸°ë¶€', 'í›„ì›', 'í˜‘ì°¬'
        ]
        
        for keyword in exclude_keywords:
            if keyword in full_title:
                logger.debug(f"ì œì™¸ í‚¤ì›Œë“œ '{keyword}'ë¡œ ì¸í•´ ì œì™¸: {title}")
                return False
        
        # 2. ë§ˆê°ì¼ì´ ìˆëŠ” ê²½ìš°
        if deadline:
            # ë§ˆê°ì¼ì´ ì˜¤ëŠ˜ë³´ë‹¤ ì´ì „ì´ë©´ ì œì™¸ (ë§ˆê°ëœ ê³µëª¨ì „)
            if deadline < today:
                logger.debug(f"ë§ˆê°ì¼ ì§€ë‚¨ìœ¼ë¡œ ì œì™¸: {deadline} - {title}")
                return False
            
            # ì‚¬ìš©ìê°€ ì„¤ì •í•œ ë‚ ì§œ ë²”ìœ„ ì²´í¬
            if from_date and deadline < from_date:
                logger.debug(f"ì‹œì‘ì¼ ì´ì „ìœ¼ë¡œ ì œì™¸: {deadline} - {title}")
                return False
            if to_date and deadline > to_date:
                logger.debug(f"ì¢…ë£Œì¼ ì´í›„ë¡œ ì œì™¸: {deadline} - {title}")
                return False
            
            logger.debug(f"ë‚ ì§œ í•„í„° í†µê³¼: {deadline} - {title}")
        else:
            # ë§ˆê°ì¼ì´ ì—†ëŠ” ê²½ìš° - ê¸°ê°„ í…ìŠ¤íŠ¸ì—ì„œ "ë§ˆê°" í‚¤ì›Œë“œ ì²´í¬
            period_text = contest_info.get('ê¸°ê°„', '').lower()
            if any(keyword in period_text for keyword in ['ë§ˆê°', 'ì¢…ë£Œ', 'ì™„ë£Œ']):
                logger.debug(f"ë§ˆê° í‚¤ì›Œë“œë¡œ ì œì™¸: {period_text} - {title}")
                return False
            
            logger.debug(f"ë§ˆê°ì¼ ì—†ìŒìœ¼ë¡œ í¬í•¨: {title}")
        
        return True
    
    def __del__(self):
        """ì†Œë©¸ì"""
        if self.driver:
            self.driver.quit()

# í¸ì˜ í•¨ìˆ˜
def crawl_wevity(keyword="ê³µê³µë°ì´í„°", max_pages=5, from_date=None, to_date=None) -> pd.DataFrame:
    """Wevity ê³µëª¨ì „ í¬ë¡¤ë§ í¸ì˜ í•¨ìˆ˜"""
    crawler = WevityCrawler()
    try:
        return crawler.crawl(keyword, max_pages, from_date, to_date)
    except Exception as e:
        logger.error(f"í¬ë¡¤ë§ ì‹¤íŒ¨: {e}")
        return pd.DataFrame()

# í…ŒìŠ¤íŠ¸ í•¨ìˆ˜
def test_crawler():
    """í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸"""
    print("ğŸ§ª Wevity í¬ë¡¤ëŸ¬ í…ŒìŠ¤íŠ¸ ì‹œì‘...")
    
    df = crawl_wevity(keyword="ê³µê³µë°ì´í„°", max_pages=2)
    
    if df.empty:
        print("âŒ í¬ë¡¤ë§ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        print(f"âœ… {len(df)}ê°œì˜ ê³µëª¨ì „ì„ ìˆ˜ì§‘í–ˆìŠµë‹ˆë‹¤.")
        print("\nğŸ“‹ ìˆ˜ì§‘ëœ ë°ì´í„° ìƒ˜í”Œ:")
        print(df.head().to_string())

if __name__ == "__main__":
    test_crawler()